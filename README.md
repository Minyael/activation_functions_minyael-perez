# README - Funciones de activaci√≥n

---

## Descripci√≥n general

Este proyecto tiene como objetivo visualizar diferentes funciones de activaci√≥n utilizadas en redes neuronales, junto con sus respectivas derivadas, haciendo uso de librer√≠as como **NumPy** y **Matplotlib**. Cada funci√≥n se encuentra implementada en un archivo independiente dentro del directorio `src/`, y el archivo `main.py` se encarga de ejecutar todas las visualizaciones en secuencia. Este proyecto fue realizado con Python 3.12.

---

## Requisitos Previos

Antes de ejecutar el c√≥digo, aseg√∫rate de tener instaladas las siguientes bibliotecas:

```bash
pip install numpy matplotlib
```

Opcionalmente, puedes usar un entorno virtual para aislar dependencias:

```bash
python -m venv env
source env/bin/activate  # En Linux/macOS
env\Scripts\activate  # En Windows
pip install -r requirements.txt
```

---

## Estructura del Proyecto

```
üìÇ activation_functions/
‚îú‚îÄ‚îÄ üìÇ src/                      # C√≥digos fuente
‚îÇ   ‚îú‚îÄ‚îÄ escalon.py              
‚îÇ   ‚îú‚îÄ‚îÄ gaussiana.py
‚îÇ   ‚îú‚îÄ‚îÄ identidad.py
‚îÇ   ‚îú‚îÄ‚îÄ lineal_a_tramos.py
‚îÇ   ‚îú‚îÄ‚îÄ relu.py
‚îÇ   ‚îú‚îÄ‚îÄ sigmoidal.py
‚îÇ   ‚îú‚îÄ‚îÄ sinusoidal.py
‚îÇ   ‚îú‚îÄ‚îÄ tangente_hiperbolica.py
‚îú‚îÄ‚îÄ main.py                       # Script principal para ejecutar el modelo
‚îú‚îÄ‚îÄ requirements.txt              # Dependencias del proyecto
‚îú‚îÄ‚îÄ README.md                     # Documentaci√≥n del proyecto
```

---

## Funciones implementadas

### Funci√≥n escal√≥n

Devuelve 1 si la entrada es mayor o igual a 0, y 0 en caso contrario. Su derivada es 0 en todos los puntos, excepto en la discontinuidad.

```python
def step_function(x):
    return np.where(x >= 0, 1, 0)
```

### Funci√≥n gaussiana

Define una campana centrada en 0 con la ecuaci√≥n \( e^{-x^2} \). Su derivada es \( -2x e^{-x^2} \).

```python
def gaussian(x):
    return np.exp(-x**2)
```

### Funci√≥n identidad

Devuelve el mismo valor de entrada. Su derivada es siempre 1.

```python
def identity(x):
    return x
```

### Funci√≥n lineal a tramos

Funci√≥n definida por tramos, con valores constantes en los extremos y una secci√≥n lineal en el centro. Su derivada es 1 en la regi√≥n lineal y 0 en los extremos.

```python
def piecewise_func(x):
    return np.piecewise(x, [x < -1, (x >= -1) & (x <= 1), x > 1], [-1, lambda x: x, 1])
```

### Funci√≥n ReLu

Devuelve 0 si la entrada es negativa y \( x \) si es positiva. Su derivada es 0 para \( x < 0 \) y 1 para \( x > 0 \).

```python
def relu(x):
    return np.maximum(0, x)
```

### Funci√≥n sigmoidal

Definida como \( \frac{1}{1 + e^{-x}} \), su derivada es \( f(x)(1 - f(x)) \).

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

### Funci√≥n sinusoidal

Funci√≥n seno est√°ndar \( \sin(x) \) con derivada \( \cos(x) \).

```python
def sinusoidal(x):
    return np.sin(x)
```

### Funci√≥n tangente hiperb√≥lica

Definida como \( \tanh(x) \), su derivada es \( 1 - \tanh^2(x) \).

```python
def tanh(x):
    return np.tanh(x)
```

---

## Ejecuci√≥n

Para entrenar y evaluar el modelo, ejecuta en la terminal:

```bash
python main.py
```

---

## Conclusi√≥n

Este proyecto muestra c√≥mo funcionan las distintas funciones de activaci√≥n haciendo uso de liber√≠as como matplotlib y numpy. Estas funciones son muy √∫tiles para implementaciones en redes neuronales y otras aplicaciones.
