# README - Funciones de activaciÃ³n

---

## DescripciÃ³n general

Este proyecto tiene como objetivo visualizar diferentes funciones de activaciÃ³n utilizadas en redes neuronales, junto con sus respectivas derivadas, haciendo uso de librerÃ­as como **NumPy** y **Matplotlib**. Cada funciÃ³n se encuentra implementada en un archivo independiente dentro del directorio `src/`, y el archivo `main.py` se encarga de ejecutar todas las visualizaciones en secuencia. Este proyecto fue realizado con Python 3.12.

---

## Requisitos Previos

Antes de ejecutar el cÃ³digo, asegÃºrate de tener instaladas las siguientes bibliotecas:

```bash
pip install numpy matplotlib
```

Opcionalmente, puedes usar un entorno virtual para aislar dependencias:

```bash
python -m venv env
source env/bin/activate  # En Linux/macOS
env\Scripts\activate  # En Windows
pip install -r requirements.txt
```

---

## Estructura del Proyecto

```
ðŸ“‚ activation_functions/
â”œâ”€â”€ ðŸ“‚ src/                      # CÃ³digos fuente
â”‚   â”œâ”€â”€ escalon.py              
â”‚   â”œâ”€â”€ gaussiana.py
â”‚   â”œâ”€â”€ identidad.py
â”‚   â”œâ”€â”€ lineal_a_tramos.py
â”‚   â”œâ”€â”€ relu.py
â”‚   â”œâ”€â”€ sigmoidal.py
â”‚   â”œâ”€â”€ sinusoidal.py
â”‚   â”œâ”€â”€ tangente_hiperbolica.py
â”œâ”€â”€ main.py                       # Script principal para ejecutar el modelo
â”œâ”€â”€ requirements.txt              # Dependencias del proyecto
â”œâ”€â”€ README.md                     # DocumentaciÃ³n del proyecto
```

---

## Funciones implementadas

### FunciÃ³n escalÃ³n

Devuelve 1 si la entrada es mayor o igual a 0, y 0 en caso contrario. Su derivada es 0 en todos los puntos, excepto en la discontinuidad.

```python
def step_function(x):
    return np.where(x < 0, 0, 1)
```

### FunciÃ³n gaussiana

Define una campana centrada en 0 con la ecuaciÃ³n \( e^{-x^2} \). Su derivada es \( -2x e^{-x^2} \).

```python
def gaussian(x):
    return (1 / np.sqrt(2 * np.pi)) * np.exp(-x**2 / 2)
```

### FunciÃ³n identidad

Devuelve el mismo valor de entrada. Su derivada es siempre 1.

```python
def identidad(x):
    return x
```

### FunciÃ³n lineal a tramos

FunciÃ³n definida por tramos, con valores constantes en los extremos y una secciÃ³n lineal en el centro. Su derivada es 1 en la regiÃ³n lineal y 0 en los extremos.

```python
def piecewise_func(x):
    return np.piecewise(x, [x < -1, (x >= -1) & (x <= 1), x > 1], [-1, lambda x: x, 1])
```

### FunciÃ³n ReLu

Devuelve 0 si la entrada es negativa y \( x \) si es positiva. Su derivada es 0 para \( x < 0 \) y 1 para \( x > 0 \).

```python
def relu(x):
    return np.maximum(0, x)
```

### FunciÃ³n sigmoidal

Definida como \( \frac{1}{1 + e^{-x}} \), su derivada es \( f(x)(1 - f(x)) \).

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

### FunciÃ³n sinusoidal

FunciÃ³n seno estÃ¡ndar \( \sin(x) \) con derivada \( \cos(x) \).

```python
def sin_func(x):
    return np.sin(x)
```

### FunciÃ³n tangente hiperbÃ³lica

Definida como \( \tanh(x) \), su derivada es \( 1 - \tanh^2(x) \).

```python
def tanh_func(x):
    return np.tanh(x)
```

---

## EjecuciÃ³n

Para entrenar y evaluar el modelo, ejecuta en la terminal:

```bash
python main.py
```

---

## ConclusiÃ³n

Este proyecto muestra cÃ³mo funcionan las distintas funciones de activaciÃ³n haciendo uso de liberÃ­as como matplotlib y numpy. Estas funciones son muy Ãºtiles para implementaciones en redes neuronales y otras aplicaciones.
